# Federated Learning with Kafka and Spark

This project demonstrates distributed machine learning using Federated Learning, Apache Kafka for messaging, and Apache Spark for stream processing.

## Architecture

- **Device Layer**: Simulated IoT sensors generating temperature and vibration data
- **Fog Layer**: Local Spark nodes training models on streaming data
- **Cloud Layer**: Central aggregator combining models using Federated Averaging

## Setup

1. Install requirements: `pip install -r requirements.txt`
2. Start Kafka: `docker-compose up -d`
3. Create topics (see commands in documentation)
4. Run components in order:
   - Producers
   - Fog nodes
   - Aggregator

## Technologies

- Python 3.8+
- Apache Kafka
- Apache Spark (PySpark)
- Docker

## Project Structure
```
federated-learning-project/
â”œâ”€â”€ producers/       # Sensor simulators
â”œâ”€â”€ fog_nodes/       # Local training nodes
â”œâ”€â”€ cloud/          # Model aggregator
â”œâ”€â”€ models/         # ML model definitions
â”œâ”€â”€ utils/          # Helper functions
â”œâ”€â”€ dashboard/      # Monitoring
â””â”€â”€ logs/           # Application logs
```

## Status

ðŸš§ In Development
```

4. Save

---

## **Part F: Your Current Project Structure**

Your VS Code Explorer should now show:
```
federated-learning-project/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_imports.py
â”œâ”€â”€ cloud/
â”œâ”€â”€ dashboard/
â”œâ”€â”€ fog_nodes/
â”œâ”€â”€ logs/
â”œâ”€â”€ models/
â”œâ”€â”€ producers/
â””â”€â”€ utils/